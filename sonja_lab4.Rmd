---
title: "R Notebook"
output: html_notebook
---

# get a language

```{r}
library(dplyr)
library(summarytools)
library(caret)
library("ggpubr")
require(investr)

languages <- c("Arabic", "Basque", "Catalan", "Chinese", "Czech", "English", "Greek", "Hungarian", "Italian", "Turkish")

# read a language file
read_language <- function(lang) {
  
  path <- paste("./data/",paste(lang,"_dependency_tree_metrics.txt",sep = ""),sep = "")
  raw_data <- read.table(path,header = FALSE,sep = " ",fill = TRUE,quote="", comment.char="") 
  colnames(raw_data) = c("vertices","degree_2nd_moment", "mean_length")
  raw_data = raw_data[order(raw_data$vertices),]
  
  # check validity
  valid_data <- raw_data %>% mutate(check = case_when(        
                                          (degree_2nd_moment >= (4-6)/vertices & degree_2nd_moment <= vertices-1) ~ "valid",
                                          TRUE ~ "invalid")) %>% filter(check == "valid") %>% select(-check)
  return(valid_data)
}


# plot data with line at mean+4*sd
# return outlier free dataframe if remove=TRUE
find_outliers <- function(data,remove) {
  
  mean <- mean(data$degree_2nd_moment)
  sd <- sd(data$degree_2nd_moment)
  
  plot(data$vertices, data$degree_2nd_moment, xlab = "vertices", ylab = "degree 2nd moment")
  abline(h=mean+4*sd, col='green')
  
  # count
  n_outliers <- length(which(data$degree_2nd_moment > mean+4*sd))
  print(paste("outliers removed for",lang,":",n_outliers))
  
  if (remove == TRUE) {
    # remove
    data <- data %>% mutate(outlier = case_when(
      degree_2nd_moment > mean+4*sd ~ "out",
      TRUE ~ "in" )) %>% filter(outlier == "in") %>% select(-outlier)
    
    return(data)
  }
  
}

# plot model 1
plot_model1 <- function(data,b_1) {
  plot(data$vertices, data$degree_2nd_moment, xlab = "vertices", ylab = "degree 2nd moment")
  lines(data$vertices, (data$vertices/2)^b_1 , col = "red")
  lines(data$vertices,4-6/data$vertices, col = "blue")
  lines(data$vertices,data$vertices-1, col = "blue")
}


remove_influential <- function(data, model) {
  jack_1 <- nlsJack(model)
  dfbetas <- jack_1$dfb
  threshold <- 2/sqrt(nrow(data))
  
  influential <- which(dfbetas>threshold)
  data <- data[-influential,]
  return(data)
}

# plot model with CI
plot_with_CI <- function(data, nls_model, conf_level) {
  plotFit(nls_model, interval = "confidence", level = conf_level, data, adjust= 'none',
        shade = TRUE, col.conf = "pink", col.fit = "red")
  lines(data$vertices,4-6/data$vertices, col = "blue")
  lines(data$vertices,data$vertices-1, col = "blue")
}
```


# preliminary plot

With a line at 3 sd from the mean to identify outliers.

```{r}
lang <- read_language("Czech")
setEPS()
postscript(paste("outliers.eps",sep='_'),width = 7, height = 3.5)
data <- find_outliers(lang,remove=TRUE)
dev.off()
```
We remove them and plot the data again. 

```{r}
plot(data$vertices, data$degree_2nd_moment, xlab = "vertices", ylab = "degree 2nd moment")
```


We suspect heteroskedasticity.
Thus, we aggregate the data grouping on the number of vertices.
This works very well for short sentences, but as we can see from the boxplot sentences longer than 60 words circa are more rare, meaning that the smoothing effect of avaraging will not be in place, and aggregation will not lead to an accurate approximation of the k second moment. 

```{r}
mean_data <- aggregate(data, list(data$vertices), mean)

setEPS()
postscript(paste("long_sentences.eps",sep='_'),width = 7, height = 3.5)
par(mfrow=c(1,2))
plot(mean_data$vertices, mean_data$degree_2nd_moment, xlab = "vertices", ylab = "degree 2nd moment")
abline(v=50, col= 'red')
boxplot(data$vertices)
abline(h=50, col= 'red')
dev.off()

```



Run the first model

```{r}
# MODEL 1

## Obtain initial parameters with linear regression
linear_model_1 = lm(log(degree_2nd_moment)~log(vertices), mean_data)
b_initial = coef(linear_model_1)[2]

## Perform nonlinear regression
nonlinear_model_1 = nls(degree_2nd_moment~(vertices/2)^b, data = mean_data,
                      start = list(b = b_initial), trace = FALSE)

## Get AIC, RSS and obtained coefficients
s_1 <- round(sqrt(deviance(nonlinear_model_1)/df.residual(nonlinear_model_1)),4)
AIC_1 <- round(AIC(nonlinear_model_1),4)
b_1 <- round(coef(nonlinear_model_1)["b"],4)

plot_with_CI(mean_data,nonlinear_model_1,0.95)

```


### correlation residual-fitted

```{r}

corrs_agg <- c()
for (i in 1:length(languages)) {
  lang <- languages[i]
  data_raw <- read_language(lang)
  data <- find_outliers(data_raw, remove=T)
  data <- aggregate(data, list(data$vertices), mean)
  
  ## Obtain initial parameters with linear regression
  linear_model_1 = lm(log(degree_2nd_moment)~log(vertices), data)
  b_initial = coef(linear_model_1)[2]
  
  ## Perform nonlinear regression
  nonlinear_model_1 = nls(degree_2nd_moment~(vertices/2)^b, data = data,
                        start = list(b = b_initial), trace = FALSE)
  residuals <- nlsResiduals(nonlinear_model_1)
  res <- data.frame(residuals$resi1)
  corrs_agg[i] <- cor(res$Fitted.values,res$Residuals)
}

corrs_agg

```



We could try to downweight the observations corresponding to sentences longer than 50 words, since they have a strong influence on the model, but they are not as representative as the first ones. 
We can see a plot of the DFBETAS computed with the Jackknife procedure, confirming the previous statement.

```{r}
require(nlstools)
jack_1 <- nlsJack(nonlinear_model_1)
dfbetas <- jack_1$dfb
threshold <- 2/sqrt(nrow(mean_data))

setEPS()
postscript(paste("dfbetas.eps",sep='_'),width = 7, height = 3.5)
plot(mean_data$vertices, dfbetas, xlab = "vertices", ylab = "jackknife DFBETAS")
abline(h=threshold, col= 'blue')
dev.off()
```



# PLOTS

```{r}

languages <- c("Arabic", "Basque", "Catalan", "Chinese", "Czech", "English", "Greek", "Hungarian", "Italian", "Turkish")
langs_2p <- c("Arabic", "Basque", "Catalan", "English", "Greek", "Hungarian", "Italian")
langs_0 <- c("Chinese","Turkish")
langs_1p <- c("Czech")
    
    for (l in 1:length(languages)) {
      lang <- languages[l]
      data <- read_language(lang)
      
      # remove outliers
      data <- find_outliers(data,remove = T)
      
      # aggregate
      mean_lang = aggregate(data, list(data$vertices), mean)
      
      if (lang %in% langs_2p) {
        model <- "2_p"
        
        d_initial <- 0
        model_2plus = nls(degree_2nd_moment~a*(vertices)^b + d, data=mean_lang, 
                    start = list(a = a_initial, b = b_initial, d = d_initial),
                    control = list(maxiter=1000),algorithm = 'port',
                    lower=c(0,0,-20),
                    upper=c(20,5,20))
        
        setEPS()
        postscript(paste(lang,model,"screened.eps",sep='_'))
        plot_with_CI(mean_lang,model_2plus,0.95)
        dev.off()
      }
      
        if (lang %in% langs_0) {
        model <- "0"
        
        setEPS()
        postscript(paste(lang,model,"screened.eps",sep='_'))
        plot(mean_lang$vertices, mean_lang$degree_2nd_moment, xlab = "vertices", ylab = "degree 2nd moment")
        lines(mean_lang$vertices,(1 - 1/mean_lang$vertices)*(5 - 6/mean_lang$vertices), col = "red")
        lines(mean_lang$vertices,4-6/mean_lang$vertices, col = "blue")
        lines(mean_lang$vertices,mean_lang$vertices-1, col = "blue")
        dev.off()
      }
        
        else if (lang %in% langs_1p) {
        model <- "1p"
        d_initial <- 7
        model_1plus = nls(degree_2nd_moment~(vertices/2)^b + d, data=mean_lang, 
                          start = list(b = b_initial, d = d_initial))
        
        setEPS()
        postscript(paste(lang,model,"screened.eps",sep='_'))
        plot_with_CI(mean_lang,model_1plus,0.95)
        dev.off()
      }
    }

```



